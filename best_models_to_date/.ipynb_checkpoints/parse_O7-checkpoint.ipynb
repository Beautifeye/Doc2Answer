{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import torchvision.transforms as T\n",
    "import shutil\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "MODEL_PATH = \"/home/ubuntu/storage/Doc2Answer/handigit/text_detection/trained_models/checkpoint-29.pt\"\n",
    "MIN_CONFIDENCE = 0.6\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Stamps detection and classification for SocialCard documents')\n",
    "parser.add_argument('-i','--input_img',  help='path to the doc to parse', required=True)\n",
    "parser.add_argument('-o','--output_dir', help='path to the directory to save all outputs to', required=True)\n",
    "args = parser.parse_args()\n",
    "\n",
    "if os.path.isdir(args.output_dir):\n",
    "    shutil.rmtree(args.output_dir)\n",
    "os.mkdir(args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, img_paths):\n",
    "        self.img_paths = img_paths\n",
    "        self.transforms = T.Compose([T.ToTensor()])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
    "        return self.transforms(x)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "\n",
    "class HandDigitModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HandDigitModel, self).__init__()\n",
    "        self.feature_extractor = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=False)        \n",
    "        self.linear = nn.Sequential(nn.Linear(1000, 256), nn.ReLU(), nn.Linear(256, 11*6))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        out = self.linear(x)\n",
    "        return out.view(len(x), 6, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(MODEL_PATH)\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "ds = CellDataset([args.input_img])\n",
    "dl = torch.utils.data.DataLoader(ds, 1)\n",
    "\n",
    "raw_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in dl:\n",
    "        raw_preds += model(batch.to(DEVICE))\n",
    "preds = raw_preds[0]\n",
    "        \n",
    "img = cv2.cvtColor(cv2.imread(args.input_img), cv2.COLOR_BGR2RGB)\n",
    "scores = preds[\"scores\"].cpu().numpy()\n",
    "above_threshold_ids = scores > MIN_CONFIDENCE\n",
    "scores = scores[above_threshold_ids]\n",
    "boxes = preds[\"boxes\"].cpu().numpy()[above_threshold_ids]\n",
    "\n",
    "colors = [tuple([int(np.random.rand() * 255) for _ in range(3)]) for _ in boxes]\n",
    "for color, score, box in zip(colors, scores, boxes):\n",
    "    x1, y1, x2, y2 = box.astype(int)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), color, 3)\n",
    "cv2.imwrite(os.path.join(args.output_dir, \"text-detection.jpg\"), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = A.Compose([\n",
    "    A.Resize(128, 128)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "model = HandDigitModel()\n",
    "model.load_state_dict(torch.load('./text_recognition/HandDigitNet-checkpoint-v1.pt', map_location=DEVICE))\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "print('Model loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.cvtColor(cv2.imread(args.input_img), cv2.COLOR_RGB2GRAY)\n",
    "img_bgr = cv2.imread(args.input_img)\n",
    "all_preds_data = {k:[] for k in [\"box\", \"text\"]}\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for color, box in zip(colors, boxes):\n",
    "    x1, y1, x2, y2 = box.astype(int)\n",
    "    patch = img[y1:y2, x1:x2]\n",
    "    X_i = test_transforms(image=patch)['image']\n",
    "    X_i = T.ToTensor()(X_i)\n",
    "    X_i = torch.cat([X_i]*3, dim=0)[None]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_i.to(DEVICE)).detach().cpu().numpy()\n",
    "        \n",
    "    pred_digits = np.argmax(preds[0], axis=1)\n",
    "    string_result = ''.join([str(el) for i, el in enumerate(pred_digits) if 10 not in pred_digits[:i+1]])\n",
    "    if len(string_result)==6:\n",
    "        string_result = string_result[:2]+'-'+string_result[2:4]+'-'+string_result[4:]\n",
    "    \n",
    "    all_preds_data[\"box\"].append([x1, y1, x2, y2])\n",
    "    all_preds_data[\"text\"].append(string_result)\n",
    "    \n",
    "    cv2.rectangle(img_bgr, (x1, y1), (x2, y2), color, 3)\n",
    "    cv2.putText(img_bgr, string_result if string_result != \"\" else \"X\", (x1, y1 + (y2 - y1) // 2), cv2.FONT_HERSHEY_PLAIN, 3, color)\n",
    "\n",
    "cv2.imwrite(os.path.join(args.output_dir, \"text-detextion-extraction.jpg\"), img_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_latest_p36]",
   "language": "python",
   "name": "conda-env-pytorch_latest_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
