{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "KeyboardInterrupt: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-40736491f44a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malbumentations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0malbum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mToTensorV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: KeyboardInterrupt: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import albumentations as album\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "\n",
    "# from tablenet import MarmotDataModule\n",
    "from tablenet import TableNetModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from albumentations import Compose\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class O7Dataset(Dataset):\n",
    "    \"\"\"O7 Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data: List[Path], transforms: Compose = None) -> None:\n",
    "        \"\"\"O7 Dataset initialization.\n",
    "\n",
    "        Args:\n",
    "            data (List[Path]): A list of Path.\n",
    "            transforms (Optional[Compose]): Compose object from albumentations.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Dataset Length.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Get sample data.\n",
    "\n",
    "        Args:\n",
    "            item (int): sample id.\n",
    "\n",
    "        Returns (Tuple[tensor, tensor, tensor]): Image, Table Mask, Column Mask\n",
    "        \"\"\"\n",
    "        sample_id = self.data[item].stem\n",
    "\n",
    "        image_path  = self.data[item]\n",
    "#         table_path = self.data[item].parent.parent.joinpath(\"table_mask\", sample_id + \".bmp\")\n",
    "#         column_path = self.data[item].parent.parent.joinpath(\"column_mask\", sample_id + \".bmp\")\n",
    "        table_path  = os.path.join(\"/home/ubuntu/storage/Doc2Answer/handigit/OCR_tablenet/masks/table/\",   str(image_path).split('/')[-1])\n",
    "        column_path = os.path.join(\"/home/ubuntu/storage/Doc2Answer/handigit/OCR_tablenet/masks/columns/\", str(image_path).split('/')[-1])\n",
    "\n",
    "        image = np.array(Image.open(image_path))\n",
    "        table_mask = np.expand_dims(np.array(Image.open(table_path)), axis=2)\n",
    "        column_mask = np.expand_dims(np.array(Image.open(column_path)), axis=2)\n",
    "        mask = np.concatenate([table_mask, column_mask], axis=2) / 255\n",
    "        sample = {\"image\": image, \"mask\": mask}\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image=image, mask=mask)\n",
    "\n",
    "        image = sample[\"image\"]\n",
    "        mask_table = sample[\"mask\"][:, :, 0].unsqueeze(0)\n",
    "        mask_column = sample[\"mask\"][:, :, 1].unsqueeze(0)\n",
    "        return image, mask_table, mask_column\n",
    "\n",
    "\n",
    "class O7DataModule(pl.LightningDataModule):\n",
    "    \"\"\"Pytorch Lightning Data Module for O7.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str = \"./data\", transforms_preprocessing: Compose = None,\n",
    "                 transforms_augmentation: Compose = None, batch_size: int = 8, num_workers: int = 4):\n",
    "        \"\"\"O7  Data Module initialization.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Dataset directory.\n",
    "            transforms_preprocessing (Optional[Compose]): Compose object from albumentations applied\n",
    "             on validation an test dataset.\n",
    "            transforms_augmentation (Optional[Compose]): Compose object from albumentations applied\n",
    "             on training dataset.\n",
    "            batch_size (int): Define batch size.\n",
    "            num_workers (int): Define number of workers to process data.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data = list(Path(data_dir).rglob(\"*.jpg\"))\n",
    "        self.transforms_preprocessing = transforms_preprocessing\n",
    "        self.transforms_augmentation = transforms_augmentation\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        \"\"\"Start training, validation and test datasets.\n",
    "\n",
    "        Args:\n",
    "            stage (Optional[str]): Used to separate setup logic for trainer.fit and trainer.test.\n",
    "        \"\"\"\n",
    "        n_samples = len(self.data)\n",
    "        self.data.sort()\n",
    "        train_slice = slice(0, int(n_samples * 0.8))\n",
    "        val_slice = slice(int(n_samples * 0.8), int(n_samples * 0.9))\n",
    "        test_slice = slice(int(n_samples * 0.9), n_samples)\n",
    "\n",
    "        self.complaint_train = O7Dataset(self.data[train_slice], transforms=self.transforms_augmentation)\n",
    "        self.complaint_val = O7Dataset(self.data[val_slice], transforms=self.transforms_preprocessing)\n",
    "        self.complaint_test = O7Dataset(self.data[test_slice], transforms=self.transforms_preprocessing)\n",
    "\n",
    "    def train_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        \"\"\"Create Dataloader.\n",
    "\n",
    "        Returns: DataLoader\n",
    "        \"\"\"\n",
    "        return DataLoader(self.complaint_train, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        \"\"\"Create Dataloader.\n",
    "\n",
    "        Returns: DataLoader\n",
    "        \"\"\"\n",
    "        return DataLoader(self.complaint_val, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        \"\"\"Create Dataloader.\n",
    "\n",
    "        Returns: DataLoader\n",
    "        \"\"\"\n",
    "        return DataLoader(self.complaint_test, batch_size=self.batch_size, num_workers=self.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (896, 896)\n",
    "transforms_augmentation = album.Compose([\n",
    "    album.Resize(1024, 1024, always_apply=True),\n",
    "    album.RandomResizedCrop(*image_size, scale=(0.7, 1.0), ratio=(0.7, 1)),\n",
    "    album.HorizontalFlip(),\n",
    "    album.VerticalFlip(),\n",
    "    album.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "transforms_preprocessing = album.Compose([\n",
    "    album.Resize(*image_size, always_apply=True),\n",
    "    album.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "complaint_dataset = O7DataModule(\n",
    "    data_dir=\"/home/ubuntu/storage/Doc2Answer/download_from_drive/data/ProcessedO7/\",\n",
    "    transforms_preprocessing=transforms_preprocessing,\n",
    "    transforms_augmentation=transforms_augmentation,\n",
    "    batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TableNetModule(batch_norm=False)\n",
    "\n",
    "EXPERIMENT_NAME = f\"{model.__class__.__name__}\"\n",
    "logger = TensorBoardLogger('tb_logs', name=EXPERIMENT_NAME)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor='validation_loss', save_top_k=5, save_last=True, mode=\"min\")\n",
    "early_stop_callback = EarlyStopping(monitor='validation_loss', mode=\"min\", patience=10)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    callbacks=[lr_monitor, checkpoint_callback, early_stop_callback],\n",
    "    logger=logger,\n",
    "    max_epochs=5,\n",
    "    gpus=1 if torch.cuda.is_available() else None\n",
    ")\n",
    "trainer.fit(model, datamodule=complaint_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_latest_p36]",
   "language": "python",
   "name": "conda-env-pytorch_latest_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
