{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "VALID_IMGS_PATH = \"valid_imgs.pkl\"\n",
    "MODEL_PATH = \"trained_models/checkpoint-29.pt\"\n",
    "MIN_CONFIDENCE = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_paths):\n",
    "        self.img_paths = img_paths\n",
    "        self.transforms = T.Compose([T.ToTensor()])\n",
    "    def __getitem__(self, idx):\n",
    "        x = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
    "        return self.transforms(x)\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_imgs = pickle.load(open(VALID_IMGS_PATH, \"rb\"))\n",
    "\n",
    "model = torch.load(MODEL_PATH)\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "ds = CellDataset(valid_imgs)\n",
    "dl = torch.utils.data.DataLoader(ds, 1)\n",
    "\n",
    "raw_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dl, total=len(dl)):\n",
    "        raw_preds += model(batch.to(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_ids = np.random.randint(0, len(valid_imgs), size=15)\n",
    "for img_path, preds in zip(np.array(valid_imgs)[rnd_ids], np.array(raw_preds)[rnd_ids]):\n",
    "    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    scores = preds[\"scores\"].cpu().numpy()\n",
    "    above_threshold_ids = scores > MIN_CONFIDENCE\n",
    "    \n",
    "    scores = scores[above_threshold_ids]\n",
    "    boxes = preds[\"boxes\"].cpu().numpy()[above_threshold_ids]\n",
    "    for score, box in zip(scores, boxes):\n",
    "        x1, y1, x2, y2 = box.astype(int)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), tuple([int(np.random.rand() * 255) for _ in range(3)]), 5)\n",
    "    \n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_latest_p36]",
   "language": "python",
   "name": "conda-env-pytorch_latest_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
