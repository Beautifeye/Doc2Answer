{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./faster_RCNN/')\n",
    "\n",
    "from cell_dataset import CellDataset\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_transform(train, augmentation_dict=None):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        for aug_key, factors in augmentation_dict.items():\n",
    "            transforms.append(T.DataAugmentation(0.5, aug_key, factors[0], factors[1]))\n",
    "        transforms.append(T.ToTensor())\n",
    "#         transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    else:\n",
    "        transforms.append(T.ToTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_PATH = \"cellDetection_annotations.pkl\"\n",
    "IMG_DIR  = \"../../download_from_drive/data/ProcessedO7\"\n",
    "AUGMENTATION_DICT = {\n",
    "    'contrast': [0.5, 1.5],\n",
    "    'brightness': [0.4, 1.5],\n",
    "    'saturation': [0.4, 1.2],\n",
    "    'gamma': [1, 1.25],\n",
    "    'hue': [-0.1, 0.1]\n",
    "}\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "ann_df = pd.read_pickle('cellDetection_annotations.pkl')\n",
    "ann_df[\"path\"] = ann_df.filename.apply(lambda fn: os.path.join(IMG_DIR, fn.split(\"/\")[-1].split(\".json\")[0]))\n",
    "ann_df.drop([\"filename\", \"value\"], axis=1, inplace=True)\n",
    "\n",
    "imgs = ann_df.path.unique()\n",
    "train_imgs, valid_imgs = train_test_split(imgs, train_size=0.7, random_state=1234)\n",
    "\n",
    "train_df = ann_df.query(\"path in @train_imgs\")\n",
    "valid_df = ann_df.query(\"path in @valid_imgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = get_transform(train=True,  augmentation_dict=AUGMENTATION_DICT)\n",
    "valid_transforms = get_transform(train=False, augmentation_dict=None)\n",
    "\n",
    "train_ds = CellDataset(data_df=train_df, transforms=train_transforms)\n",
    "valid_ds = CellDataset(data_df=valid_df, transforms=valid_transforms)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=os.cpu_count(),\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "valid_dl = torch.utils.data.DataLoader(\n",
    "    dataset=valid_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=os.cpu_count(),\n",
    "    collate_fn=utils.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_dir = 'trained_models'\n",
    "if os.path.isdir(checkpoints_dir):\n",
    "    shutil.rmtree(checkpoints_dir)\n",
    "os.mkdir(checkpoints_dir)\n",
    "\n",
    "# our dataset has two classes only - background and CELL\n",
    "num_classes = 2\n",
    "model = build_model(num_classes)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "# number of epochs\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, train_dl, DEVICE, epoch, print_freq=10)\n",
    "    \n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, valid_dl, device=DEVICE)\n",
    "    \n",
    "    # save model afeter the current epoch\n",
    "    torch.save(model, os.path.join(checkpoints_dir, 'checkpoint-' + str(epoch).zfill(2)) + '.pt')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(valid_imgs, open(\"valid_imgs.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_latest_p36]",
   "language": "python",
   "name": "conda-env-pytorch_latest_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
